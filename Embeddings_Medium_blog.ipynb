{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK0B9qp_hJbd"
      },
      "source": [
        "# Get LlamaIndex and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uK6E0GEEbYlv"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6TCnMgk9R_0V"
      },
      "outputs": [],
      "source": [
        "import os, itertools\n",
        "import llama_index\n",
        "import openai\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.core import StorageContext, Settings, SimpleKeywordTableIndex, VectorStoreIndex\n",
        "from llama_index.core import node_parser\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import retrievers\n",
        "from llama_index.core import load_index_from_storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SFcpMmJ_Zn-o"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_KEY'] = [YOUR OPENAI KEY HERE]\n",
        "openai.api_key = os.environ['OPENAI_KEY']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6YJa8OyxLnS"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ztiLzLIxJO9"
      },
      "outputs": [],
      "source": [
        "# Download president_wikipedia_articles from https://github.com/nathanbos/blog_embeddings and save them locally\n",
        "documents = SimpleDirectoryReader(\"president_wikipedia_articles\").load_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KKjbP3Rj9F"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5GQdRWvEVm"
      },
      "source": [
        "# Get embedding models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkf16R9Y6Aow"
      },
      "outputs": [],
      "source": [
        "# Get OpenAI embeddings. Will only work if you entered and OpenAI key above..\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "ada_embed_model = OpenAIEmbedding(model = \"text-embedding-ada-002\")  # default\n",
        "large_embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vAcVf83Sz11x"
      },
      "outputs": [],
      "source": [
        "# get some more embedding models from huggingface\n",
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI6n4h0Fz15F"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "bge_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "st_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Rgl8pCgjt9"
      },
      "source": [
        "## Test the embed models (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xs_fW_83874"
      },
      "outputs": [],
      "source": [
        "# Could use the embed model directly like this\n",
        "large_embedding = large_embed_model.get_text_embedding(\"Who was President during World War 2?\")\n",
        "bge_embedding = bge_embed_model.get_text_embedding(\"Who was President during World War 2?\")\n",
        "st_embedding = st_embed_model.get_text_embedding(\"Who was President during World War 2?\")\n",
        "ada_embedding = ada_embed_model.get_text_embedding(\"Who was President during World War 2?\")\n",
        "print(len(bge_embedding)) # confirm we've got the right embedding\n",
        "print(len(st_embedding)) # confirm we've got the right embedding\n",
        "print(len(ada_embedding)) # confirm we've got the right embedding\n",
        "print(len(large_embedding)) # confirm we've got the right embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI3p8qPoDf57"
      },
      "source": [
        "### Set chunk size to 128.\n",
        "128 is very small for precise context matches; 512 would work better for a real RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcKhPMgrDigV"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "Settings.text_splitter = SentenceSplitter(chunk_size=128, chunk_overlap=24)\n",
        "Settings.chunk_size = 128\n",
        "Settings.chunk_overlap = 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkkdqIFLx9d6"
      },
      "source": [
        "# Define indexes\n",
        "### Indexes are the databases- they store vectors and text passages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkQLsx9j3lNL"
      },
      "outputs": [],
      "source": [
        "# index documents with an embedding model\n",
        "bge_index128 = VectorStoreIndex.from_documents(documents, embed_model=bge_embed_model)\n",
        "st_index128 = VectorStoreIndex.from_documents(documents, embed_model=st_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKYAqe5d51Uj"
      },
      "outputs": [],
      "source": [
        "ada_index128 = VectorStoreIndex.from_documents(documents, embed_model=ada_embed_model)\n",
        "large_index128 = VectorStoreIndex.from_documents(documents, embed_model=large_embed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ9hM-950bgm"
      },
      "source": [
        "# Save indexes for future use (optional)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJQrNdbfzJOD"
      },
      "outputs": [],
      "source": [
        "bge_index128.storage_context.persist(persist_dir=\"storage/bge_people_db128\")\n",
        "st_index128.storage_context.persist(persist_dir=\"storage/st_people_db128\")\n",
        "ada_index128.storage_context.persist(persist_dir=\"storage/ada_people_db128\")\n",
        "large_index128.storage_context.persist(persist_dir=\"storage/large_people_db128\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvdpoIb202EH"
      },
      "source": [
        "# This is how you would retrieve stored indexes later (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-CZ37UJx0Dy"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import load_index_from_storage\n",
        "# Documentation:  https://docs.llamaindex.ai/en/latest/api_reference/storage/indices_save_load.html\n",
        "\n",
        "ada_context = StorageContext.from_defaults(persist_dir=\"storage/ada_people_db128\")\n",
        "ada_index128 = load_index_from_storage(ada_context)\n",
        "\n",
        "large_context = StorageContext.from_defaults(persist_dir=\"storage/large_people_db128\")\n",
        "large_index128 = load_index_from_storage(large_context)\n",
        "\n",
        "st_context = StorageContext.from_defaults(persist_dir=\"storage/st_people_db128\")\n",
        "st_index128 = load_index_from_storage(st_context)\n",
        "\n",
        "bge_context = StorageContext.from_defaults(persist_dir=\"storage/bge_people_db128\")\n",
        "bge_index128 = load_index_from_storage(bge_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X30Kk7a5vA3U"
      },
      "source": [
        "# Define Retrievers\n",
        "* Retrievers will return just the matching chunks; if you want an LLM to use these to formulate a response use query engines, below.\n",
        "* The k parameter controls how many context nodes get returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wdgHfHGEvTI"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "large_retriever128 = VectorIndexRetriever(index=large_index128, embed_model=large_embed_model, similarity_top_k=20)\n",
        "ada_retriever128 = VectorIndexRetriever(index=ada_index128, embed_model=ada_embed_model, similarity_top_k=20)\n",
        "st_retriever128 = VectorIndexRetriever(index=st_index128, embed_model=st_embed_model, similarity_top_k=20)\n",
        "bge_retriever128 = VectorIndexRetriever(index=bge_index128, embed_model=bge_embed_model, similarity_top_k=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZdpzg8b_uYv"
      },
      "outputs": [],
      "source": [
        "## Test one\n",
        "test = st_retriever128.retrieve(\"Which US President's name rhymes with Sarac Rom-Com-A?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS6A4LbZvmAi"
      },
      "outputs": [],
      "source": [
        "for node in test:\n",
        "  print(node.node.text)\n",
        "  print(\"---------------- END OF NODE -------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB1A1gGQWjZ6"
      },
      "source": [
        "# Or create query engines\n",
        "The k parameter controls how many context nodes get returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW95c2L9u_NG"
      },
      "outputs": [],
      "source": [
        "# define query engine from indexes\n",
        "bge_query_engine128 = bge_index128.as_query_engine(embed_model=bge_embed_model,similarity_top_k=20)\n",
        "st_query_engine128 = st_index128.as_query_engine(embed_model=st_embed_model, similarity_top_k=20)\n",
        "ada_query_engine128 = ada_index128.as_query_engine(embed_model=ada_embed_model,similarity_top_k=20)\n",
        "large_query_engine128 = large_index128.as_query_engine(embed_model=large_embed_model, similarity_top_k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyaS6wrHXG5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roZHzf2RR-t5"
      },
      "source": [
        "# Read in some questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B3Kw9qnFhuI"
      },
      "outputs": [],
      "source": [
        "# prompt: Read in an Excel file 'Questions.xlsx' to a Pandas data frame 'qs'\n",
        "\n",
        "import pandas as pd\n",
        "qs = pd.read_excel('president_questions.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWmUdxgcqFA_"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a new questionID variable in qs sequentially numbered from 1\n",
        "\n",
        "qs['questionID'] = range(1, len(qs) + 1)\n",
        "\n",
        "# create a dictionary of qnums and questions\n",
        "question_dict = qs.set_index('questionID')['Question'].to_dict()\n",
        "\n",
        "\n",
        "# If you want to use retrievers to just get returned context chunks\n",
        "query_retrievers = {\n",
        "    \"bge\": bge_retriever128,\n",
        "    \"st\": st_retriever128,\n",
        "    \"ada\": ada_retriever128,\n",
        "    \"large\": large_retriever128\n",
        "}\n",
        "\n",
        "# If you want to use query engines with an LLM\n",
        "query_engines = {\n",
        "    \"st\": st_query_engine128,\n",
        "    \"bge\": bge_query_engine128,\n",
        "    \"ada\": ada_query_engine128,\n",
        "    \"large\": large_query_engine128,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvsCunvPSJlL"
      },
      "source": [
        "# Query with retrievers (just get context passages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_70-xmNbSFXz"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "# Initialize an empty list to hold your data\n",
        "data = []\n",
        "\n",
        "for qnum, question in question_dict.items():\n",
        "    try:\n",
        "      print(f\"question number: {qnum}\")\n",
        "      # Loop through each model in the dictionary\n",
        "      for model, retriever in query_retrievers.items():\n",
        "        print(model)\n",
        "        print(f\"Model: {retriever}\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "        # Get the response from the query engine\n",
        "        response = retriever.retrieve(question)\n",
        "        filenames = [node.node.metadata['file_name'] for node in response]\n",
        "        chunks_returned = [node.node.text for node in response]\n",
        "        scores = [node.score for node in response]\n",
        "        sources = [f\"{filename}\\n score:{score}\\n text: {chunk}\" for filename, chunk, score in zip(filenames, chunks_returned, scores)]\n",
        "        sources += [''] * (20 - len(sources))  # Pad the list if it has less than 10 sources\n",
        "        # Construct the row with qNum, embed_model, llm_response, and sources\n",
        "        row = [str(qnum), question, model] + sources\n",
        "        print(row)\n",
        "        # Append the row to your data list\n",
        "        data.append(row)\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred with model {model} for question {qnum}: {e}\")\n",
        "# Define column names for your DataFrame\n",
        "column_names = ['qNum', 'question', 'embed_model'] + [f'source_{i}' for i in range(1, 21)]\n",
        "\n",
        "# Create the DataFrame\n",
        "output = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "output.to_excel('President_answers.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTPh6OvoStVy"
      },
      "source": [
        "# Query with index (get LLM response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FU4SliyW1z65"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "# Initialize an empty list to hold your data\n",
        "data = []\n",
        "\n",
        "#for qnum, question in islice(question_dict.items(), 2):\n",
        "for qnum, question in question_dict.items():\n",
        "    try:\n",
        "      print(f\"question number: {qnum}\")\n",
        "      for model, engine in query_engines.items():\n",
        "        print(model)\n",
        "        # Get the response from the query engine\n",
        "        response = engine.query(question)\n",
        "        llm_response = response.response  # Example access to a hypothetical response part\n",
        "        filenames = [node.metadata['file_name'] for node in response.source_nodes if 'file_name' in node.metadata]\n",
        "        chunks_returned = [node.text for node in response.source_nodes]\n",
        "        sources = [f\"{filename}: {chunk}\" for filename, chunk in zip(filenames, chunks_returned)]\n",
        "        sources += [''] * (20 - len(sources))  # Pad the list if it has less than 20 sources\n",
        "        # Construct the row with qNum, embed_model, llm_response, and sources\n",
        "        row = [str(qnum), question, model, llm_response] + sources\n",
        "        print(row)\n",
        "        # Append the row to your data list\n",
        "        data.append(row)\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred with model {model} for question {qnum}: {e}\")\n",
        "# Define column names for your DataFrame\n",
        "column_names = ['qNum', 'question', 'embed_model', 'llm_response'] + [f'source_{i}' for i in range(1, 21)]\n",
        "\n",
        "# Create the DataFrame\n",
        "output = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "output.to_excel('President_answers.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5DyQ5RumDw5"
      },
      "source": [
        "# If you want to get your own Wikipedia article set\n",
        "* Will need a Wikipedia account\n",
        "* Can substitute a different list for presidents. Make sure the names match the Wikipedia article names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N98t7c6vmM-h"
      },
      "outputs": [],
      "source": [
        "presidents = [\"John Adams\", \"John Quincy Adams\", \"Chester A. Arthur\", \"Joe Biden\", \"James Buchanan\", \"George H. W. Bush\",\n",
        " \"George W. Bush\", \"Jimmy Carter\", \"Grover Cleveland\", \"Bill Clinton\", \"Calvin Coolidge\", \"Dwight D. Eisenhower\",\n",
        " \"Millard Fillmore\", \"Gerald Ford\", \"James A. Garfield\", \"Ulysses S. Grant\", \"Warren G. Harding\", \"Benjamin Harrison\",\n",
        " \"William Henry Harrison\", \"Rutherford B. Hayes\", \"Herbert Hoover\", \"Andrew Jackson\", \"Thomas Jefferson\", \"Andrew Johnson\",\n",
        " \"Lyndon B. Johnson\", \"John F. Kennedy\", \"Abraham Lincoln\", \"James Madison\", \"William McKinley\", \"James Monroe\", \"Richard Nixon\",\n",
        " \"Barack Obama\", \"Franklin Pierce\", \"James K. Polk\", \"Ronald Reagan\", \"Franklin D. Roosevelt\", \"Theodore Roosevelt\",\n",
        " \"William Howard Taft\", \"Zachary Taylor\", \"Harry S. Truman\", \"Donald Trump\", \"John Tyler\", \"Martin Van Buren\",\n",
        " \"George Washington\", \"Woodrow Wilson\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAKHTjHFmNvu"
      },
      "outputs": [],
      "source": [
        "def fetch_wikipedia_page(title, user_agent):\n",
        "    \"\"\"\n",
        "    Fetches the content of a Wikipedia page given its title.\n",
        "    \"\"\"\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True\n",
        "    }\n",
        "    headers = {\n",
        "        'User-Agent': user_agent\n",
        "    }\n",
        "    response = requests.get(URL, headers=headers, params=params)\n",
        "    data = response.json()\n",
        "    page = next(iter(data['query']['pages'].values()))\n",
        "    return page['extract'] if 'extract' in page else \"No content available\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Hd-MAvmSv2"
      },
      "outputs": [],
      "source": [
        "def save_text(filename, content):\n",
        "    \"\"\"\n",
        "    Saves the given content to a text file with the specified filename.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6nkxbE6mTpX"
      },
      "outputs": [],
      "source": [
        "# User agent\n",
        "user_agent = [YOUR WIKIPEDIA ACCOUNT INFO HERE]\n",
        "\n",
        "# Directory to save the files\n",
        "save_dir = 'president_wikipedia_articles'\n",
        "\n",
        "# Process each president\n",
        "for president in presidents:\n",
        "    filename = f\"{president.replace('.', '').replace(' ', '_')}.txt\"\n",
        "    path = os.path.join(save_dir, filename)\n",
        "    content = fetch_wikipedia_page(president, user_agent)\n",
        "    save_text(path, content)\n",
        "    print(f\"Saved: {path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
